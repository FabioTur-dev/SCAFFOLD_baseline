# ğŸ”¬ Federated Learning Baselines â€” Multi-Round Experiments

This repository contains controlled implementations of classical **multi-round Federated Learning (FL)** baselines used for rigorous comparison with one-shot methods (e.g., GH-OFL).

The goal is strict experimental fairness across methods.

---

## ğŸ“Œ Implemented Methods

- **FedAvg**
- **FedProx (Î¼ = 0.01)**
- **SCAFFOLD (weighted, fair implementation)**

---

## ğŸ“Š Evaluated Datasets

- **CIFAR-10**
- **CIFAR-100**
- **SVHN**

All experiments use **Dirichlet client partitions** with:

```
Î± âˆˆ {0.5, 0.1, 0.05}
```

to simulate different levels of statistical heterogeneity.

---

## ğŸ“‚ Repository Structure

```
SCAFFOLD_baseline/
â”‚
â”œâ”€â”€ fedavg_c10_fast.py
â”œâ”€â”€ fedavg_c100.py
â”œâ”€â”€ fedavg_svhn.py
â”‚
â”œâ”€â”€ fedprox_c10.py
â”œâ”€â”€ fedprox_c100.py
â”œâ”€â”€ fedprox_svhn.py
â”‚
â”œâ”€â”€ scaffold.py
â”‚
â”œâ”€â”€ visual.py              # Generates plots (results are hardcoded)
â”‚
â”œâ”€â”€ *.pdf                  # Generated plots
â””â”€â”€ data/                  # Automatically downloaded datasets
```

---

## âš™ï¸ Experimental Configuration (IDENTICAL across methods)

All baselines share the same hyperparameters to guarantee strict comparability:

- **Backbone:** ResNet-18 (ImageNet pretrained)
- **Clients:** 10
- **Local epochs:** 1
- **Batch size:** 256
- **Rounds:** 50
- **Optimizer:** SGD (momentum = 0.9)
- **Learning rate:** 0.001
- **Seed:** 42

This ensures that performance differences arise from the **algorithm**, not tuning.

---

## â— Why LOCAL_EPOCHS = 1?

This choice is intentional.

It:

- Ensures direct comparability with one-shot methods
- Forces multi-round FL into a constrained regime
- Highlights communication efficiency differences
- Reveals behavior under minimal local training

Under this setting, drift-correction methods (especially SCAFFOLD) operate in a limited regime.

---

## â–¶ï¸ How to Run Experiments

Example:

```bash
python fedavg_c100.py
python fedprox_svhn.py
python scaffold.py
```

Datasets are automatically downloaded into `./data`.

---

## ğŸ“ˆ Plot Generation

`visual.py` generates plots by hardcoding the baseline results.

It does **not** run training â€” it only produces figures from stored values.

Example:

```bash
python visual.py
```

---

## ğŸ” Implementation Notes

- FedAvg and FedProx use **weighted aggregation**.
- SCAFFOLD uses weighted updates for both model parameters and control variates.
- Architectures are dataset-consistent.
- No hyperparameter advantages are given to any method.
- Strict fairness is enforced across all baselines.

---

## ğŸ§ª Research Context

These baselines are designed for controlled comparison with one-shot federated methods.  
They intentionally operate under constrained local training to highlight:

- Communication cost differences  
- Sensitivity to non-IID heterogeneity  
- Convergence stability  
- Algorithmic robustness  

---

**Maintainer:** Fabio Turazza  
PhD Student â€” Federated Learning & One-Shot Methods
